{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15e55f-0625-4640-9078-6efec41f5604",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install mediapipe tensorflow opencv-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1666e-8c84-4c9a-a062-3ef05a6dae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import uuid\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c3efaa-9f7e-4905-9fe1-682c112c05fd",
   "metadata": {},
   "source": [
    "Set Up Variables and Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f94d3f6-6f3f-45f2-9c3a-770958e3cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "# Data Collection Variables\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Desktop/MP_Data') \n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "\n",
    "\n",
    "# for action in actions: \n",
    "#     for sequence in range(no_sequences):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353bcb52-1908-472c-8631-ab92a7465ecc",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcd1c3a4-949a-4cf4-af77-e578364cc9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    model.load_weights('action.h5')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 300+num*40), (int(prob*100), 20+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 285+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c744ae-c8a5-445f-b928-73d1ae136d11",
   "metadata": {},
   "source": [
    "Function for testing capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28093fb-2ce3-47eb-91b7-91a2baefa604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "    timer = 3\n",
    "    is_taking_photo = False\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            if is_taking_photo:\n",
    "                if count % 15 == 0:\n",
    "                    print(timer)\n",
    "                    timer -= 1\n",
    "                count += 1\n",
    "                if count % 45 == 0:\n",
    "                    print(\"Snap\")\n",
    "                    cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "                    is_taking_photo = False\n",
    "                    timer = 3\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            # print(results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                is_taking_photo = True\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c46c65f-eae9-42ac-88a2-c0979d4893ad",
   "metadata": {},
   "source": [
    "Function for capturing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0db259-4363-4313-9a44-98eb4901da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_data():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        # NEW LOOP\n",
    "        # Loop through actions\n",
    "        for action in actions:\n",
    "            # Loop through sequences aka videos\n",
    "            for sequence in range(no_sequences):\n",
    "                # Loop through video length aka sequence length\n",
    "                for frame_num in range(sequence_length):\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "    #                 print(results)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "                    \n",
    "                    # NEW Apply wait logic\n",
    "                    if frame_num == 0: \n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120,500), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(1000)\n",
    "                    else: \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                    # NEW Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf4d39a-7258-4308-9f9a-33290e797a7f",
   "metadata": {},
   "source": [
    "Function to train model off captured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce38ac0e-1a15-47bc-8381-149f2a62f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(needsShape):\n",
    "    label_map = {label:num for num, label in enumerate(actions)}\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            window = []\n",
    "            for frame_num in range(sequence_length):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                window.append(res)\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "    \n",
    "    X = np.array(sequences)\n",
    "    y = to_categorical(labels).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "    \n",
    "    log_dir = os.path.join('Logs')\n",
    "    tb_callback = TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    if needsShape:\n",
    "        return input_shape\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=150, callbacks=[tb_callback])\n",
    "    model.summary()\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    print(multilabel_confusion_matrix(ytrue, yhat))\n",
    "    print(accuracy_score(ytrue, yhat))\n",
    "    \n",
    "    model.save('action.h5')\n",
    "\n",
    "    return input_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bb2a4-9739-4aab-9993-e284f0da7266",
   "metadata": {},
   "source": [
    "Function for using model to make real time predictions - Note that the dimensions are camera specific, as such the graphics to show prediction probabilities may be sized incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7adfe703-6b27-456c-981c-3f42dac3772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def predictions(input_shape):\n",
    "    colour = 0\n",
    "    count = 0\n",
    "    timer = 3\n",
    "    is_taking_photo = False\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.load_weights('action.h5')\n",
    "\n",
    "    # 1. New detection variables\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    threshold = 0.8\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            if is_taking_photo:\n",
    "                if count % 15 == 0:\n",
    "                    print(timer)\n",
    "                    timer -= 1\n",
    "                count += 1\n",
    "                if count % 45 == 0:\n",
    "                    print(\"Snap\")\n",
    "                    cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "                    is_taking_photo = False\n",
    "                    timer = 3\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "    #         sequence.insert(0,keypoints)\n",
    "    #         sequence = sequence[:30]\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                colour = colors[np.argmax(res)]\n",
    "                \n",
    "                \n",
    "            #3. Viz logic\n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 1: \n",
    "                    sentence = sentence[-1:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, actions, image)\n",
    "\n",
    "            if colour == 0:\n",
    "                cv2.rectangle(image, (0,0), (1920, 200), (245, 117, 16), -1)\n",
    "            else:\n",
    "                cv2.rectangle(image, (0,0), (1920, 200), colour, -1)\n",
    "                \n",
    "            cv2.putText(image, ' '.join(sentence), (3,175), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                is_taking_photo = True\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "                \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4fd3a1-d4ba-4536-ab9d-037c9e5e9a1a",
   "metadata": {},
   "source": [
    "Running the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe14f0-222d-4b9d-9a5c-7d6b957ce70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa740f66-0b98-483b-9894-ac9d5b42169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ba16b-1bef-4ba9-ae2e-251671b87188",
   "metadata": {},
   "source": [
    "train_data() can sometimes output a poor model - 30% accuracy. If this happens, rerun to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605af817-2cd4-4813-8e6c-177bbdb03334",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68e0bd-3433-47da-a4fe-05db3303cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = train_data(True)\n",
    "predictions(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52215c40-4ec3-49b3-af9e-c33c2e2f5d90",
   "metadata": {},
   "source": [
    "To Run everything again after resetting kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ad89ce-4b9d-4823-b6b7-c18701e1ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import uuid\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities\n",
    "\n",
    "# Data Collection Variables\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Desktop/MP_Data') \n",
    "# Actions that we try to detect\n",
    "actions = np.array(['hello', 'thanks', 'iloveyou'])\n",
    "# Thirty videos worth of data\n",
    "no_sequences = 30\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "\n",
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "\n",
    "\n",
    "# for action in actions: \n",
    "#     for sequence in range(no_sequences):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "\n",
    "\n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "\n",
    "\n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(30,1662)))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    model.load_weights('action.h5')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def prob_viz(res, actions, input_frame):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0, 300+num*40), (int(prob*100), 20+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 285+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame\n",
    "\n",
    "\n",
    "def capture():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    count = 0\n",
    "    timer = 3\n",
    "    is_taking_photo = False\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            if is_taking_photo:\n",
    "                if count % 15 == 0:\n",
    "                    print(timer)\n",
    "                    timer -= 1\n",
    "                count += 1\n",
    "                if count % 45 == 0:\n",
    "                    print(\"Snap\")\n",
    "                    cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "                    is_taking_photo = False\n",
    "                    timer = 3\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            # print(results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                is_taking_photo = True\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def capture_data():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        \n",
    "        # NEW LOOP\n",
    "        # Loop through actions\n",
    "        for action in actions:\n",
    "            # Loop through sequences aka videos\n",
    "            for sequence in range(no_sequences):\n",
    "                # Loop through video length aka sequence length\n",
    "                for frame_num in range(sequence_length):\n",
    "\n",
    "                    # Read feed\n",
    "                    ret, frame = cap.read()\n",
    "\n",
    "                    # Make detections\n",
    "                    image, results = mediapipe_detection(frame, holistic)\n",
    "    #                 print(results)\n",
    "\n",
    "                    # Draw landmarks\n",
    "                    draw_styled_landmarks(image, results)\n",
    "                    \n",
    "                    # NEW Apply wait logic\n",
    "                    if frame_num == 0: \n",
    "                        cv2.putText(image, 'STARTING COLLECTION', (120,500), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0,255, 0), 4, cv2.LINE_AA)\n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                        cv2.waitKey(1000)\n",
    "                    else: \n",
    "                        cv2.putText(image, 'Collecting frames for {} Video Number {}'.format(action, sequence), (15,100), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                        # Show to screen\n",
    "                        cv2.imshow('OpenCV Feed', image)\n",
    "                    \n",
    "                    # NEW Export keypoints\n",
    "                    keypoints = extract_keypoints(results)\n",
    "                    npy_path = os.path.join(DATA_PATH, action, str(sequence), str(frame_num))\n",
    "                    np.save(npy_path, keypoints)\n",
    "\n",
    "                    # Break gracefully\n",
    "                    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                        cap.release()\n",
    "                        cv2.destroyAllWindows()\n",
    "                        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def train_data(needsShape):\n",
    "    label_map = {label:num for num, label in enumerate(actions)}\n",
    "    \n",
    "    sequences, labels = [], []\n",
    "    for action in actions:\n",
    "        for sequence in range(no_sequences):\n",
    "            window = []\n",
    "            for frame_num in range(sequence_length):\n",
    "                res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                window.append(res)\n",
    "            sequences.append(window)\n",
    "            labels.append(label_map[action])\n",
    "    \n",
    "    X = np.array(sequences)\n",
    "    y = to_categorical(labels).astype(int)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)\n",
    "    \n",
    "    log_dir = os.path.join('Logs')\n",
    "    tb_callback = TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "    input_shape = X_train.shape[1:]\n",
    "    if needsShape:\n",
    "        return input_shape\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=150, callbacks=[tb_callback])\n",
    "    model.summary()\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "    yhat = np.argmax(yhat, axis=1).tolist()\n",
    "    print(multilabel_confusion_matrix(ytrue, yhat))\n",
    "    print(accuracy_score(ytrue, yhat))\n",
    "    \n",
    "    model.save('action.h5')\n",
    "\n",
    "    return input_shape\n",
    "\n",
    "def predictions(input_shape):\n",
    "    colour = 0\n",
    "    count = 0\n",
    "    timer = 3\n",
    "    is_taking_photo = False\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))\n",
    "    model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "    model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(actions.shape[0], activation='softmax'))\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.load_weights('action.h5')\n",
    "\n",
    "    # 1. New detection variables\n",
    "    sequence = []\n",
    "    sentence = []\n",
    "    threshold = 0.8\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    # Set mediapipe model \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while cap.isOpened():\n",
    "\n",
    "            if is_taking_photo:\n",
    "                if count % 15 == 0:\n",
    "                    print(timer)\n",
    "                    timer -= 1\n",
    "                count += 1\n",
    "                if count % 45 == 0:\n",
    "                    print(\"Snap\")\n",
    "                    cv2.imwrite(os.path.join('Output Images', '{}.jpg'.format(uuid.uuid1())), image)\n",
    "                    is_taking_photo = False\n",
    "                    timer = 3\n",
    "\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mediapipe_detection(frame, holistic)\n",
    "            print(results)\n",
    "            \n",
    "            # Draw landmarks\n",
    "            draw_styled_landmarks(image, results)\n",
    "            \n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "    #         sequence.insert(0,keypoints)\n",
    "    #         sequence = sequence[:30]\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-30:]\n",
    "            \n",
    "            if len(sequence) == 30:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                colour = colors[np.argmax(res)]\n",
    "                \n",
    "                \n",
    "            #3. Viz logic\n",
    "                if res[np.argmax(res)] > threshold: \n",
    "                    if len(sentence) > 0: \n",
    "                        if actions[np.argmax(res)] != sentence[-1]:\n",
    "                            sentence.append(actions[np.argmax(res)])\n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 1: \n",
    "                    sentence = sentence[-1:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, actions, image)\n",
    "\n",
    "            if colour == 0:\n",
    "                cv2.rectangle(image, (0,0), (1920, 200), (245, 117, 16), -1)\n",
    "            else:\n",
    "                cv2.rectangle(image, (0,0), (1920, 200), colour, -1)\n",
    "                \n",
    "            cv2.putText(image, ' '.join(sentence), (3,175), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == ord('s'):\n",
    "                is_taking_photo = True\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "                \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
